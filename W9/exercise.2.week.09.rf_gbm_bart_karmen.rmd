---
title: "Week 09 Statistical Machine Learning in Bioinformatics: Tree based methods"
author: "Palle Villesen"
date: "Last update: `r Sys.Date()`"
output: 
  html_document:
    code_folding: show
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r, warning=F, message=F}

library(tidyverse)
theme_set(theme_classic())


library(randomForest)
library(gbm)
library(BART)

```


# Aim of exercise

For any practical purpose you will probably never use single regression/classification trees. So in this exercise we will NOT fit single trees.

Instead you should familiarize yourself with the three ensembl methods from chapter 9:

* RandomForest
* Boosting
* BART (Bayesian something)

We will first focus on doing regression (age) on the Karmen data. You worked on these with Lasso and Ridge regression.

After you are done you can repeat the exercise - but focus on sex (classification) instead.

Basically we have measured ~400 molecules in blood from healthy donors and we can try to predict the sex or age of the donor. 

Sex should be easy - age more difficult. For this exercise we will focus on the age as output/response.

# Do the book labs!!!

Before you continue, it is super important that you go through the book labs (8.3)

They will give you the tools needed to do this exercise.

.
.
.


Have you done the book labs? If so, you should be able to continue here.


# Data wrangling

## Reading and reformatting data

I code as as 0 and 1 and make it a factor.

```{r}

df  <- read_rds("karmen_data_with_sex_and_age.rds") 
compounds  <- df %>% select(-age,-sex) %>% names()

df$sex <- factor(df$sex-1)

df <- df %>% select(age,sex,everything())

(sex_data <- df %>% select(sex,all_of(compounds)))
(age_data <- df %>% select(age, all_of(compounds)))

rm(df)

```

## Create a training/test split

```{r}

set.seed(0)

train <- sample(x = 1:nrow(age_data), size = round(0.8*nrow(age_data)))

age_train <- age_data[train,]
age_test  <- age_data[-train,]

```

## Data inspection

Inspect the different variables. 

## Q1.1: How many predictors do we have?

## Q1.2: What is your intuition about the associations between the data and the reponse?

Think: do you expect linear relationships? Interactions? How many of the predictors do you think are associated with the two different responses?

Questions like these may guide you in the direction of the model of choice!

## Q1.3: What is the size of the train/test split?

# RandomForest regression of age

## Q2.1: Use the labs and train a random forest on the training set using 500 trees

randomForest(,..do.trace=T) will track progress

Read the documentation for the randomForest() function.

Also: there are many randomforest packages out there - for convenience we stick to the same one as the book.

The randomforest algorithm selects random subsets of variables and samples, if you all use set.seed(0) before calling the function, then you will all get identical results. This may be useful ;)

Personally I don't like the "formula interface" for theese models, but do as you please.

I would also advice you to look at the help for randomForest() and look at the arguments.

Especially the x,y,xtest and ytest...

AND!!!! Look at the "Value" section of the help. Look what kind of error measures you get back from the model...

For regression: mse and "test" may be very interesting!

And a note: you need to set keep.forest = T... (to keep the forest).

```{r}

library(randomForest)

set.seed(0)

xtrain <- age_train %>% select(-age)
ytrain <- age_train$age
xtest  <- age_test %>% select(-age)
ytest  <- age_test$age


```


---

## Q2.2: Make a plot with tree number on x and estimated mse on y

I normally code stuff like this:

```r

rffit <- randomForest(x = xtrain, y=ytrain, keep.forest = T,
                    xtest=xtest, ytest=ytest,
                    ntree = 500, importance=T,
                    do.trace = T
                    )
                    
names(rffit)


```


---

## Q2.3: Do you have enough trees in your forest?

Look at the plot - does it look like the error stabilizes?

If so - ok.

If not - you need more trees!


---

## Q2.4: How is the importance distributed?

I.e. does it look like many or few molecules are important?


---

## Changing mtry (aka tuning the random forest)

### Q2.5: Try different values for the mtry parameter and evaluate how it influences the estimated and test error rate

mtry = Number of variables randomly sampled as candidates at each split. 

Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)

What happens if you use all off them? (bagging)
What happens if you use very few predictors for each split? (e.g. 1/10 of predictors)


---

# Boosting regression of age

Again, go through the book labs (if you haven't done so.)

## Q3.1: Use the labs and train a gbm() on the data set using 1000 trees

Read the documentation for the gbm() function.

Also: there are many gbm packages out there - for convenience we stick to the same one as the book.

A big difference is the tendency to (sometimes) overfit. 

To avoid this we do CV and get an estimate of the test error. This is built into the gbm() function - but it's NOT mentioned in the book labs.

And... now it get's annoying: gbm does NOT use the x and y interface... It uses a formula only.

```{r}

library(gbm)

set.seed(0)

```


---

Like the randomforest() function, gbm() also returns some info on train and c.v. error.

Look at the help for gbm and identify what you need! (You have to look at what is returned)

## Q3.2: Plot the CV error as function of tree number


---

## Q3.3: What is your conclusion here? How many trees would you choose to use?


---


## Q3.4: We have used shallow trees (depth 1) - what if you use deeper trees?


---

# BART regression

I will NOT ask you to do BART. 

I have never used it for any practical purpose and it hasn't gained a lot of attention in ML competitions.

I will show you a quick fit (stolen from the book labs) and how we can calculate the test MSE.

Then you can compare the test MSE with the test MSE from your fitted randomForest and GBM.

And then... maybe I should reconsider and start using BART? You tell me!

```{r}

library(BART)

bartfit   <- gbart(x.train = as.matrix(xtrain) , ytrain , x.test = as.matrix(xtest))
yhat.bart <- bartfit$yhat.test.mean
mse_bart  <- mean (( ytest - yhat.bart)^2)

bart_predicted <- predict(bartfit, newdata = as.matrix(xtest))
test_predicted <- colMeans(bart_predicted)
test_observed  <- ytest
residuals_bart <- (test_observed - test_predicted)
mean(residuals_bart^2)

```

# Comparing the 3 methods using the test data

## Q4.1: Use your three models to predict the test data, compare the error (MSE, MAE, RMSE - whatever your prefer). 


---

# Well done! 

As you can see all three methods are somewhat similar in syntax.

But also different - some can use a formula input, some need a formula, some can't use formula.

Also, tuning of the parameters is difficult and requires deep knowledge of all the parameters.

BUT!!! We do have a package that unifies everything.

It makes it very easy to run any kind of test error estimation (Cross Validation, Bootstrapping, etc) on any method (lasso, randomforest, boosting, ridge - whatever). 

It also always use the same input data format.

It also have the same way to get "importance" back from a model.

It's called the "caret" package - and we will get to that in 1-2 weeks.

There are some higher speed versions of random forest for R (ranger) and boosting (xgboost) that runs a LOT faster than the packages used in the book labs. We will come back to these. (Both of these are availble in caret).

And if you want super speed you should look for "lightgbm".


## Notes on tuning

We haven't really covered the many different parameters of randomforests, GBM and BART. If you look at the book and the functions you can see that there are quite a lot of parameters that can/will influence the train and test error.

There is only one way to go: tuning the models using cross validation.

Caret can also tune all the parameters using bootstrapping, cross validation, repeat cross validation or whatever you prefer...

Stay tuned!

# >BONUS_Q: Try sex for all three methods!

If you want - go ahead. Warning: you will run into errors and frustrations.


---
